<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Summary of Machine Learning Algorithms -- Support Vector Machine</title><meta name="description" content="Support Vector Machine (SVM) has become a more and more popular algorithm in the field of ML, even some times more than Neural Network (NN). It is widely use..."><link rel="shortcut icon" type="image/x-icon" href="/public/favicon.ico"><link rel="stylesheet" href="/public/css/main.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.5.0/css/font-awesome.css" integrity="sha512-EaaldggZt4DPKMYBa143vxXQqLq5LE29DG/0OoVenoyxDrAScYrcYcHIuxYO9YNTIQMgD8c8gIUU8FQw7WpXSQ==" crossorigin="anonymous"><link rel="canonical" href="https://simcookies.github.io/2018/12/02/support-vector-machine"><link rel="alternate" type="application/rss+xml" title="Simcookies" href="https://simcookies.github.io/feed.xml"><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" integrity="sha512-uto9mlQzrs59VwILcLiRYeLKPPbS/bT71da/OEBYEwcdNUk8jYIy+D176RYoop1Da+f9mvkYrmj5MCLZWEtQuA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.7.12/simple-jekyll-search.min.js" integrity="sha512-APy7Ff/y4pdIHleqiTMcRZ8Wu6tjfqhJpgAcaCvTuFQPj/G+/A5u0uZi/DkfkuLQ6FeFmDJgh/zl0y3If/VUyA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.2/mermaid.min.js" integrity="sha512-x8NWYlEsC86ngfO24tbxW6pMuyn9gYnwEW0FcSobohDc3iLCXhmRkqYXgTfE7Jwy2YCTnHRfyum8LUIiyvmgWQ==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha512-Fv0JVzxvrWFLhiUP8wZYR6VwBZQt0XqB9MhHZE+12lObqIDzg3LOJcCCiRy6g4MCX/MlG+R9IRCWZAHjo9iBgw==" crossorigin="anonymous"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      showProcessingMessages: false,
      messageStyle: "none",
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
      }
    });</script><script src="/public/script/main.js"></script></head><body><header class="site-header"><div class="wrapper"><a class="site-title" href="/">Simcookies</a> <span class="menu-icon" id="menu-icon"><span class="fa-stack fa-lg"><i class="fa fa-square-o fa-stack-2x"></i> <i class="fa fa-bars fa-stack-1x"></i></span></span><nav class="site-nav"><div class="trigger"><a class="page-link" href="/about/">About</a> <a class="page-link" href="/archive/">Archive</a> <a class="page-link" href="/wiki/">Wiki</a> <a class="page-link" href="/feed.xml">Feeds</a> <a class="page-link" href="https://github.com/simcookies/gitio" target="_blank" rel="noopener noreferrer"><i class="fa fa-github-alt fa-lg"></i> Repositories</a></div></nav></div></header><div class="scroll-content"><div class="page-content"><div class="wrapper"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header"><h1 class="post-title" itemprop="name headline">Summary of Machine Learning Algorithms -- Support Vector Machine</h1><p class="post-meta"><time datetime="2018-12-02T10:10:42+09:00" itemprop="datePublished">Dec 2, 2018</time>  <i class="fa fa-archive" aria-hidden="true"></i>  <span><a class="category-div" href="/category/machine-learning">machine learning</a></span>  <i class="fa fa-tag"></i>  <a class="tag-div" href="/tag/algorithm">algorithm</a>  <a class="tag-div" href="/tag/formula">formula</a> </p></header><div class="post-content" itemprop="articleBody"><p><strong>Support Vector Machine</strong> (SVM) has become a more and more popular algorithm in the field of ML, even some times more than <strong>Neural Network</strong> (NN). It is widely used for classification and regression. So, in this post I will give a brief about the SVM.</p><h1 id="brief-introduction">Brief Introduction</h1><p>SVM is a kind of supervised learning method, it has good performance for classification problems. And it is a non-probabilistic way of learning, which means it can can directly get the result of the classification instead of the probability of the class which it belongs to (like logistic regression). In addition, the support of the “<strong>Nuclear Function</strong>” in SVM also makes it have a wider range of use. The concept of “nuclear function” is also used in other classification learning methods.</p><h1 id="support-vector-machine">Support Vector Machine</h1><p>First, let’s say we have two types of data, one positive type and one negative type. Features have two dimensional data: $\vec{x} = (x_1, x_2)$, which can be plotted like below (Orange for positive data and blue for negative data):</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201102144404.png" alt="SVM original dataset"></p><p>Now we want to draw a line to separate the two types of data. There are three lines to choose from below.</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201102144443.png" alt="SVM with different classfication lines"></p><p>Obviously, the middle (b) is the best way to separate the data. So how can we count the “most able to” separate the data? We can assume a hyperplane which divide dataset into two classes. And this plane has as longer distance as possiable to all the two kinds of data. We call it **separating hyperplane **(or decision boundray). Equation for hyperplane (here is the line (b) in the figure): \(\vec w\cdot \vec x + b=0\) $\vec w$ is the vector perpendicular to the hyperplane. The magnitude of $\vec w$ is unkown. $\vec x$ are the vectors on the hyperplane. $b$ is bias.</p><p>For negative objects: \(\vec w\cdot \vec x_- + b\leq1\) For positive objects: \(\vec w\cdot \vec x_+ + b\geq1\) The thing what we want to do is <strong>very easy</strong>, it just is checking the result of: \(\text{sgn}(\vec w\cdot\vec x+b)\) If it’s plus then it belongs to positive class, otherwise it belongs to negitive class.</p><p>The big problem is how to find the unknow $\vec w$ and $b$. We can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The distance between these hyperplanes is called <strong>margin</strong>.</p><p>The two hyperplanes can be described by:</p>\[\vec w\cdot \vec x_i + b=1 \\ \vec w\cdot \vec x_i + b=-1\]<p>Here is another example (Red for positive and blue for negative). The two gray lines are the margin.</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201102144506.png" alt="svm_margin"></p><p>If $y_i$ presents the class of $i$-th piece of data, 1 for positive and -1 for negitive, then the two equations can be written into one: \(y_i(\vec w \cdot \vec x_i + b) =1\)</p><p>Those vectors in the plane are called <strong>support vector</strong>. The width between two hyperplanes:</p>\[\begin{align} \text{Width} &amp;= (\vec x_+-\vec x_-)\cdot \frac{\vec w}{||\vec w||} \notag\\ &amp;=(\vec w\cdot\vec x_+-\vec w\cdot\vec x_-)\cdot \frac{1}{||\vec w||} \notag\\ &amp;=(1-b+1+b)\cdot \frac{1}{||\vec w||} = \frac{2}{||\vec w||} \end{align}\]<p>The thing what we want to do is maxmum the width:</p>\[\text{max Width}\to\text{max}\frac{2}{||\vec w||} \to \text{min}||\vec w||\to\text{min}\frac{1}{2}||\vec w||^2\]<p>Now the problem becomes:</p>\[\text{min} \frac{1}{2}||\vec w||^2, s.t., y_i(\vec w \cdot \vec x_i + b) -1\geq0, i = 1,2\dots,n\]<p>This problem can be solved by <strong>Lagrange multiplier</strong>:</p>\[L(\vec w, b, \alpha) = \frac{1}{2}||\vec w||^2-\sum_{i=1}^{n}\alpha_i\left[y_i(\vec w\cdot\vec x_i + b)-1\right]\]<p>Now we need know partial difference $L$ with respect to $\vec w$ and $b$ and let them equal to 0:</p>\[\begin{align} \frac{\partial{L}}{\partial \vec w_i} &amp;= \vec w - \sum_{i=1}^{n}\alpha_i y_i \vec x_i = 0\notag\\ \frac{\partial{L}}{\partial b} &amp;= - \sum_{i=1}^{n}\alpha_i y_i = 0 \end{align}\]<p>We got the $\vec w$, and bring it back to the Largrange muptiplier:</p>\[\begin{align} L(\vec w, b, \alpha) &amp;= \frac{1}{2}\left(\sum_{i=1}^n\alpha_iy_i\vec x_i\right)^2-\sum_{i=1}^n\alpha_iy_i\vec x_i\left(\sum_{i=1}^n\alpha_iy_i\vec x_i\right)-\sum_{i=1}^n\alpha_iy_ib+\sum_{i=1}^n\alpha_i\notag\\ &amp;=\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec x_i\vec x_j-\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec x_i \vec x_j - b\sum_{i=1}^n\alpha_iy_i+\sum_{i=1}^n\alpha_i\notag\\ &amp;=\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\vec x_i\vec x_j \end{align}\]<p>This is a quadratic problem. The most popular way to solve the problem is sequential minimal optimization, <strong>SMO</strong>(Microsoft).</p></div><hr><div class="post-nav"><a href="/2018/09/21/summary-of-machine-learning-linear-regression" class="left link">&lt;&lt; Previous </a><a href="/2020/06/27/make-image-host-service-for-typora-with-picgo-and-gihub" class="right link">Next &gt;&gt;</a></div><hr><div id="disqus_thread"></div><script>/**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
  */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function() {  // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');

  s.src = '//simcookiesgithubio.disqus.com/embed.js';

  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="noopener noreferrer" target="_blank">comments powered by Disqus.</a></noscript></article><div class="mark-link"><a id="bookmark"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-book fa-stack-1x fa-inverse"></i></span></a></div><div id="toc-area" class="sidebar" style="display:none"><div class="col toc-area">  <i class="fa fa-bookmark" aria-hidden="true"></i>  Table of Contents<ul class="section-nav"><li class="toc-entry toc-h1"><a href="#brief-introduction">Brief Introduction</a></li><li class="toc-entry toc-h1"><a href="#support-vector-machine">Support Vector Machine</a></li></ul></div></div></div></div><footer class="site-footer">© 2016~2020 by Simcookies. All rights reserved.<div class="quick-link" id="quick-link"><a id="page-top"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-arrow-up fa-stack-1x fa-inverse"></i> </span></a><a href="/"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-home fa-stack-1x fa-inverse"></i></span></a></div></footer></div><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-85209586-1', 'auto');
  ga('send', 'pageview');</script></body></html>