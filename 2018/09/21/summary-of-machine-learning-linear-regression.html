<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Summary of Machine Learning Algorithms -- Linear Regression</title><meta name="description" content="Regression is one class of problems in Machine Learning. Linear Regression is a basic one class of problems of Regression. I want to write a note about the L..."><link rel="stylesheet" href="/public/css/main.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="canonical" href="https://simcookies.github.io/2018/09/21/summary-of-machine-learning-linear-regression"><link rel="alternate" type="application/rss+xml" title="Simcookies" href="https://simcookies.github.io/feed.xml"><script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script><script src="https://ajax.googleapis.com/ajax/libs/jqueryui/1.11.4/jquery-ui.min.js"></script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
       showProcessingMessages: false,
       messageStyle: "none",
       jax: ["input/TeX", "output/HTML-CSS"],
       tex2jax: {
           inlineMath: [ ['$','$'], ["\\(","\\)"] ],
           displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
       }
   });</script><script src="/public/script/main.js"></script><link rel="shortcut icon" type="image/x-icon" href="/public/favicon.ico"></head><body><header class="site-header"><div class="wrapper"><a class="site-title" href="/">Simcookies</a> <span class="menu-icon" id="menu-icon"><span class="fa-stack fa-lg"><i class="fa fa-square-o fa-stack-2x"></i> <i class="fa fa-bars fa-stack-1x"></i></span></span><nav class="site-nav"><div class="trigger"><a class="page-link" href="/about/">About</a> <a class="page-link" href="/archive/">Archive</a> <a class="page-link" href="/wiki/">Wiki</a> <a class="page-link" href="/feed.xml">Feeds</a> <a class="page-link" href="https://github.com/simcookies/gitio" target="_blank"><i class="fa fa-github-alt fa-lg"></i> Repositories</a></div></nav></div></header><div class="scroll-content"><div class="page-content"><div class="wrapper"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header"><h1 class="post-title" itemprop="name headline">Summary of Machine Learning Algorithms -- Linear Regression</h1><p class="post-meta"><time datetime="2018-09-21T14:53:00+09:00" itemprop="datePublished">Sep 21, 2018</time> &nbsp;<i class="fa fa-archive" aria-hidden="true"></i>&nbsp; <span><a class="category-div" href="/category/machine-learning">machine learning</a></span> &nbsp;<i class="fa fa-tag"></i>&nbsp; <a class="tag-div" href="/tag/algorithm">algorithm</a>&nbsp; <a class="tag-div" href="/tag/formula">formula</a>&nbsp;</p></header><div class="post-content" itemprop="articleBody"><p><strong>Regression</strong> is one class of problems in Machine Learning. Linear Regression is a basic one class of problems of Regression. I want to write a note about the Linear Regression in this post as the beginning of Machine Learning series.</p><hr><h1 id="brief-introduction">Brief Introduction</h1><p>Regression problems want to find the relationship between the input variables and output variables. The <em>Regression</em> was used from a 19th-Century scientist. <em>Linear Regression</em> is most basic problems of Regression. We want to make a model to describe the relationship between input and output. So let’s assume the input variables are $x_1, x_2, \dots , x_n$, and the output varibale is $y$. This formula shows the linear relationship between them:</p><script type="math/tex;mode=display">y=m_1x_1+m_2x_2 + \dots+m_nx_n+b=m^Tx+b</script><p>Here, $m_i$ is coefficient. The $m = (m_1, m_2, \dots, m_n)$ and $x = (x_1, x_2, \dots, x_n)$ are vector, the $b$ calls bias. We have the model now, what we want to do next is to find the $m$ and $b$ with given <strong>dataset</strong> and then, we can use the model to make some prediction of some unknown data.</p><hr><h1 id="the-case-of-2-dimesion">The case of 2-dimesion</h1><p>For the 2-dimesion (just one input varibale $x$ with one output variable $y$), we can get $m$ and $b$ easily. The model formula becomes $y=mx+b$ which is a very easy line and we call it the <em>best fit line</em>.<script type="math/tex">m</script>is the best fit line slope and $b$ is best fit line y-intercept.</p><p>Now, we have one dataset with $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \dots, (x^{(m)}, y^{(m)})$. Here is the plot of them:</p><p><img src="/public/image/dataset_lr.png" alt="dataset of linear regression"></p><p>We can get $m$ and $b$ quite easily with these formulas:</p><script type="math/tex;mode=display">% <![CDATA[
\begin{align}
m&=\frac{\overline x \overline y - \overline{xy}}{\overline x^2 - \overline {x^2}} \\
b&=\overline y - m\overline x
\end{align} %]]></script><p>After this, we can draw the best fit line as blue line, which is also our model. According this line, we can make prediction of $x=40$ which plot as red point:</p><p><img src="/public/image/dataset_lr_prediction.png" alt="linear regression prediction"></p><hr><h1 id="for-the-generalized-case">For the generalized case</h1><p>For the generalized case, $m$ is a vector. So we can not get it easily. Now assume the $h(x)$ is the <strong>Hypothesis</strong> model:</p><script type="math/tex;mode=display">h(x^{(i)})=m^Tx^{(i)}+b</script><p>We can use a <strong>cost function</strong> – <strong>Mean Squared Error</strong> (MSE) to evaludate this model:</p><script type="math/tex;mode=display">J(m, b) = \frac{1}{2n}\sum_{i=1}^n\left[h(x^{(i)})-y^{(i)}\right]^2</script><p>$1/2n$ here is just for caculating convenice. What we need to do it to find the $m$ and $b$ to minimize the MSE wchich also means the line we made is as close as possible to those dataset points:</p><script type="math/tex;mode=display">\min_{m, b}J(m,b)</script><p>Now the <strong>Find unknown $m$ and $b$</strong> becomes <strong>Find the min value of $J$, and get $m$ and $b$</strong>. This problem can be solved by <strong>Gradient Descent</strong> (which I want to give a another post to give some details).</p><hr><h1 id="practice-corner">Practice corner</h1><p>For the 2-D case, it’s quite easy to achieve that:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">statistics</span> <span class="kn">import</span> <span class="n">mean</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">best_fit_slope_and_intercept</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="o">*</span> <span class="n">mean</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">xs</span> <span class="o">*</span> <span class="n">ys</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">mean</span><span class="p">(</span><span class="n">xs</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span> <span class="o">-</span> <span class="n">m</span> <span class="o">*</span> <span class="n">mean</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">b</span>

<span class="c1"># train dataset
</span><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

<span class="n">m</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">best_fit_slope_and_intercept</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">regression_line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">m</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">])</span>

<span class="c1"># prediction
</span><span class="n">predict_x</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">predict_y</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="n">predict_x</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># data visualization
</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">regression_line</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">predict_x</span><span class="p">,</span> <span class="n">predict_y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div><p>For generalized case, we can use <code class="highlighter-rouge">sklearn</code> package. here is the code sketch:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Prepare some train dataset and test dataset
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">LinearRegression</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
</code></pre></div></div><p>In fact, the data preprocess is quite important step, it includes data cleaning and feature selection. That’s a big topic, so I want to make another post to give details.</p><blockquote><ul><li><a href="https://en.wikipedia.org/wiki/Linear_regression">Linear Regression (Wiki)</a></li><li><a href="https://pythonprogramming.net/regression-introduction-machine-learning-tutorial/">pythonprogramming.net</a></li></ul></blockquote></div><hr><div class="post-nav"><a href="/2018/05/05/draw-uml-with-plantuml-in-atom" class="left link">&lt;&lt; Previous </a><a href="/2018/12/02/support-vector-machine" class="right link">Next &gt;&gt;</a></div><hr><div id="disqus_thread"></div><script>/**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
  */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function() {  // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');

  s.src = '//simcookiesgithubio.disqus.com/embed.js';

  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript></article><div class="mark-link"><a id="bookmark"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-book fa-stack-1x fa-inverse"></i></span></a></div><div id="toc-area" class="sidebar" style="display:none"><div class="col toc-area">&nbsp;&nbsp;<i class="fa fa-bookmark" aria-hidden="true"></i>&nbsp; Table of Contents<ul class="section-nav"><li class="toc-entry toc-h1"><a href="#brief-introduction">Brief Introduction</a></li><li class="toc-entry toc-h1"><a href="#the-case-of-2-dimesion">The case of 2-dimesion</a></li><li class="toc-entry toc-h1"><a href="#for-the-generalized-case">For the generalized case</a></li><li class="toc-entry toc-h1"><a href="#practice-corner">Practice corner</a></li></ul></div></div></div></div><footer class="site-footer">&copy;&nbsp;2016~2019 by Simcookies. All rights reserved.<div class="quick-link" id="quick-link"><a id="page-top"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-arrow-up fa-stack-1x fa-inverse"></i> </span></a><a href="/"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-home fa-stack-1x fa-inverse"></i></span></a></div></footer></div><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-85209586-1', 'auto');
  ga('send', 'pageview');</script></body></html>