<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>机器学习算法系列 - 逻辑回归</title><meta name="description" content="逻辑回归(Logistic Regression)也是非常经典以及常用的算法. 虽然名字叫做”回归”, 但实际上是一个常用于解决分类且多是二分类问题的算法. 在一些文献中也会被称为对数回归或者最大熵分类. 在这个模型中, 使用了逻辑函数(Logit)对描述单个实验可能结果的概率进行建模."><link rel="shortcut icon" type="image/x-icon" href="/public/favicon.ico"><link rel="stylesheet" href="/public/css/main.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.5.0/css/font-awesome.css" integrity="sha512-EaaldggZt4DPKMYBa143vxXQqLq5LE29DG/0OoVenoyxDrAScYrcYcHIuxYO9YNTIQMgD8c8gIUU8FQw7WpXSQ==" crossorigin="anonymous"><link rel="canonical" href="https://simcookies.github.io/2021/05/09/summary-of-machine-learning-algorithms-logistic-regression"><link rel="alternate" type="application/rss+xml" title="Simcookies" href="https://simcookies.github.io/feed.xml"><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" integrity="sha512-uto9mlQzrs59VwILcLiRYeLKPPbS/bT71da/OEBYEwcdNUk8jYIy+D176RYoop1Da+f9mvkYrmj5MCLZWEtQuA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.7.12/simple-jekyll-search.min.js" integrity="sha512-APy7Ff/y4pdIHleqiTMcRZ8Wu6tjfqhJpgAcaCvTuFQPj/G+/A5u0uZi/DkfkuLQ6FeFmDJgh/zl0y3If/VUyA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.2/mermaid.min.js" integrity="sha512-x8NWYlEsC86ngfO24tbxW6pMuyn9gYnwEW0FcSobohDc3iLCXhmRkqYXgTfE7Jwy2YCTnHRfyum8LUIiyvmgWQ==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha512-Fv0JVzxvrWFLhiUP8wZYR6VwBZQt0XqB9MhHZE+12lObqIDzg3LOJcCCiRy6g4MCX/MlG+R9IRCWZAHjo9iBgw==" crossorigin="anonymous"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      showProcessingMessages: false,
      messageStyle: "none",
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
      }
    });</script><script src="/public/script/main.js"></script></head><body><header class="site-header"><div class="wrapper"><a class="site-title" href="/">Simcookies</a> <span class="menu-icon" id="menu-icon"><span class="fa-stack fa-lg"><i class="fa fa-square-o fa-stack-2x"></i> <i class="fa fa-bars fa-stack-1x"></i></span></span><nav class="site-nav"><div class="trigger"><a class="page-link" href="/about/">About</a> <a class="page-link" href="/archive/">Archive</a> <a class="page-link" href="/wiki/">Wiki</a> <a class="page-link" href="/feed.xml">Feeds</a> <a class="page-link" href="https://github.com/simcookies/gitio" target="_blank" rel="noopener noreferrer"><i class="fa fa-github-alt fa-lg"></i> Repositories</a></div></nav></div></header><div class="scroll-content"><div class="page-content"><div class="wrapper"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习算法系列 - 逻辑回归</h1><p class="post-meta"><time datetime="2021-05-09T16:36:31+09:00" itemprop="datePublished">May 9, 2021</time>  <i class="fa fa-archive" aria-hidden="true"></i>  <span><a class="category-div" href="/category/machine-learning">machine learning</a></span>  <i class="fa fa-tag"></i>  <a class="tag-div" href="/tag/algorithm">algorithm</a>  <a class="tag-div" href="/tag/formula">formula</a> </p></header><div class="post-content" itemprop="articleBody"><p>逻辑回归(Logistic Regression)也是非常经典以及常用的算法. 虽然名字叫做”回归”, 但实际上是一个常用于解决分类且多是二分类问题的算法. 在一些文献中也会被称为对数回归或者最大熵分类. 在这个模型中, 使用了逻辑函数(Logit)对描述单个实验可能结果的<strong>概率</strong>进行建模.</p><h1 id="逻辑回归">逻辑回归</h1><h2 id="logit-函数和-sigmoid-函数">Logit 函数和 Sigmoid 函数</h2><p>为了讨论逻辑回归中的核心也就是Logit函数, 首先要看Odds. Odds 又叫发生比或者叫几率, 指的是一个事件发生和不发生的概率的比值.</p>\[\text{odds}=\frac{p}{1-p}\notag\]<p>如果能够预测出这个Odds, 也就能够计算得到该事件发生的概率了. 而Logit函数就是对这里的 Odds 取对数.</p>\[\text{logit}(\text{p})=\log\frac{p}{1-p}\notag\]<p><strong>问题假设</strong>: 现在要预测能否通过银行的贷款审核(结果只有是否两种, 属于二分类问题), 而手头已有的数据特征量为用户的个人信息以及信用卡偿还信息等. (这里的特征量用 $x_i(i=1,2,\dots,n)$ 表示) 这里我们假设 $\text{logit}(\text{p})$ 和特征量之间服从线性关系 (至于为什么要这么假设, 到后面可以看到这个假设能够帮助我们做分类).</p>\[\begin{align*} &amp;\log\frac{p}{1-p}=\theta_0+\theta_1x_1+\theta_2x_2+\dots+\theta_nx_n=\vec{\theta}\cdot\vec{x}\\ &amp;\Rightarrow \frac{p}{1-p} = \exp(\vec{\theta}\cdot\vec{x})\\ &amp;\Rightarrow p=\frac{\exp(\vec{\theta}\cdot\vec{x})}{1+\exp(\vec{\theta}\cdot\vec{x})}=\frac{1}{1+\exp(-\vec{\theta}\cdot\vec{x})}=g(\vec{\theta}\cdot\vec{x}) \end{align*}\]<p>最后得到的这个假设函数称之为 Logistic 函数, 也叫 <strong>Sigmoid 函数</strong>.其函数图像为:</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210502173103.png" alt="sigmoid"></p><p>其横轴就是特征量的权重和, 纵轴就是该事件发生的概率. 再根据得到的概率, 设定一个阈值, 也叫决策边界(Decision Bound). 比如这里我们设置阈值为0.5, 概率大于0.5的认为该事件会发生, 也就是银行贷款能通过, 小于0.5则不能通过. 也就是说, 不同于决策树分类得到的结果, 逻辑回归不仅可以得到最终的分类结果, 还能够得到中间产物 – 属于某一个类别的概率 $p$.</p><p>在日常生活中, 我们会用到加权求和分类的方法. 给用户每个特征量一个权重之后, 计算加权求和. 如果分数超过了某一个阈值我们就认为可以放贷. 其实这个方法用类似上述的数学语言表达的话, 可以认为最后计算的总分减去阈值是真正的权重和. 这个特征量的权重和超过零的话就判断为该事件发生, 反之为不发生.这里也用到了一个假设函数, 就是<strong>阶梯函数</strong>(Step function).</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210502175321.png" alt="sigmoid_and_step"></p><p>如上图所示, 阶梯函数更加直接. 但是阶梯函数在加权和接近于0的部分会非常的敏感, 对数据的误差容忍度很低, 并且其在0处不能够微分. 相比之下, Sigmoid函数就显得温和一些, 且单调递增可微.</p><h2 id="逻辑回归的算法描述">逻辑回归的算法描述</h2><p>有了 Sigmoid 函数的加持, 从特征量 $\vec{x}$ 到对数Odds $\text{logit}(\text{p})$ 的一一映射就产生了, 问题就在于如何确定式子里的参数 $\theta$ 的值. 其实逻辑回归本质上是在用简单线性回归的预测结果去逼近对数发生比或者说对数几率, 所以在<strong>机器学习方法</strong>这本书中, 周老师更愿意把 Logistic Regression 翻译成”对数几率回归”, 简称”对率回归”. 个人感觉非常赞同! (但是基于很多人还是会用”逻辑回归”这个词, 所以本文也保持这个名称)</p><p>同线性回归一样, 为了确定参数 $\theta$ 需要定义一个代价函数, 再通过优化算法最小化代价函数. 首先假设函数(Hypothesis)为:</p>\[h_\theta(x)=\text{Sigmoid}(\theta^Tx)=\frac{1}{1+\exp(-\theta^Tx)}\notag\]<p>需要进行学习的目标参数(Parameter)为上式中的 $\theta$. 线性回归中采用的代价函数为:</p>\[J(\theta)=\frac 1 m\sum_{i=1}^{m}\frac 1 2\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2\notag\]<p>其中 $m$ 是数据的数量. 但是假设函数是 Logit 函数, 所以得到的代价函数将是非凸函数. 对于非凸函数使用梯度下降法将有可能找不到全局最小值. 这里采用对数极大似然估计可以将代价函数转换为凸函数:</p>\[J(\theta)=\frac 1 m \sum_{i=1}^m\left[-y^{(i)}\log\left(h_\theta\left(x^{(i)}\right)\right)-\left(1-y^{(i)}\right)\log\left(1-h_\theta\left(x^{(i)}\right)\right)\right]\notag\]<p>可以求得代价函数对 $\theta$ 的导数为:</p>\[\frac\partial{\partial\theta_j}J(\theta)=\frac 1 m\sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)x_j^{(i)}\notag\]<p>利用上述的导数不断地更新参数 $\theta$, 就能获得最小的代价函数 $\min_\theta J(\theta)$:</p>\[\theta_j\Leftarrow\theta_j-\alpha\frac\partial{\partial\theta_j}J(\theta)\notag\]<p>这里的 $\alpha$ 为学习速率. 有了上面的值, 算法可以被描述为:</p><p>输入: 训练数据集 D, 学习速率 $\alpha$<br>输出: 模型权重参数 $\theta$<br>步骤:</p><ol><li>计算代价函数平均梯度;</li><li>乘上学习速率;</li><li>计算更新权重参数 $\theta$;</li></ol><h1 id="基于-python-scratch-的实现">基于 Python Scratch 的实现</h1><p>为了方便理解, 可以使用 Scikit-Learn 的 <code class="language-plaintext highlighter-rouge">make_classification</code> 帮助我们生成一堆用来分类的, 简单的数据.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_informative</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
</code></pre></div></div><p>这里生成了20组拥有两个类别的数据, 特征量的数量也只有1个. 绘制成图形如下图:</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210509124118.png" alt="下载"></p><p>首先定义代价函数:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 后续要用的 sigmoid 函数
</span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>


<span class="c1"># 正向计算得到预测值
</span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">hx</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hx</span>


<span class="c1"># 代价函数
</span><span class="k">def</span> <span class="nf">cost_fuction</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="n">class1_cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">labels</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
    <span class="n">class2_cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">labels</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">class1_cost</span> <span class="o">-</span> <span class="n">class2_cost</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cost</span>
</code></pre></div></div><p>再定义用于更新权重参数的核心函数:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">-</span> <span class="n">labels</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">gradient</span> <span class="o">*=</span> <span class="n">lr</span>
    <span class="n">weights</span> <span class="o">-=</span> <span class="n">gradient</span>
    <span class="k">return</span> <span class="n">weights</span>
</code></pre></div></div><p>这里默认学习速率 lr 为 0.01. 最后定义包含核心算法的, 用于学习的函数:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">cost_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span> <span class="o">*</span> <span class="n">epochs</span><span class="p">):</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_fuction</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="n">cost_history</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">iters</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">f"Epochs:</span><span class="si">{</span><span class="n">i</span> <span class="o">//</span> <span class="n">iters</span><span class="si">}</span><span class="s">, iters: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">, cost: </span><span class="si">{</span><span class="n">cost</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">cost_history</span>
</code></pre></div></div><p>训练函数中的迭代次数不限, 而是用 Epoch 来指定. 每一个 Epoch 里面默认循环 iter = 1000 次, 并且输出每个Epoch的代价函数值, 以查看训练过程. 预先使用随机函数定义好初始的权重参数, 使用默认10次Epoch来训练.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initial_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">weights</span><span class="p">,</span> <span class="n">cost_history</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">initial_weights</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</code></pre></div></div><p>上述代码的输出为:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epochs:0, iters: 0, cost: 0.32268668797387234
Epochs:1, iters: 1000, cost: 0.1632418968511627
Epochs:2, iters: 2000, cost: 0.13241899314581226
Epochs:3, iters: 3000, cost: 0.11868723129920973
Epochs:4, iters: 4000, cost: 0.1108135413966402
Epochs:5, iters: 5000, cost: 0.10568263401138037
Epochs:6, iters: 6000, cost: 0.10206687727097812
Epochs:7, iters: 7000, cost: 0.09937998342713258
Epochs:8, iters: 8000, cost: 0.09730508677348992
Epochs:9, iters: 9000, cost: 0.09565536684950605
</code></pre></div></div><p>可以看到, 随着循环次数的增加, 代价函数的值再不断地变小. 可视化一下看一下:</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210509152656.png" alt="下载 (1)"></p><p>可以看出循环了10000次也就是10个Epoch的时候, 代价函数的值已经很小了, 就是说对于学习速率 0.01 的情况, 学习到这里就可以了. 尝试不同的学习速率:</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210509153535.png" alt="下载"></p><p>很明显的看到, 学习速率越大, 代价函数的值下降的也越快, 至少在这个例子里是这样. 对于复杂的问题的话, 合适的学习速率的选择是非常重要的. 对于 0.01 的学习速率, 得到其计算好的权重参数后, 考察一下其模型的精度. 还是用最直观的描点看:</p><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20210509153941.png" alt="下载"></p><p>对比预测的点还有实际的点, 可以看到20点中有一个点(十分接近于0)预测发生了错位(精度为95%), 虽然可以通过继续学习提高精度, 但是基本上已经算可以了. 复杂的问题中不断地学习会导致过学习问题, 这也是很常见的.</p><h1 id="基于-scikit-learn-的实现">基于 Scikit-Learn 的实现</h1><p>Scikit-Learn中提供了 <code class="language-plaintext highlighter-rouge">LogisticRegression</code> 的函数. 至于数据我们采用 Scikit-Learn 的 Toy Datasets 中的判断乳腺癌阴阳性的数据集. 并且为了便于计算, 使用最大最小值 Scaler 将特征量映射到 (0, 1) 区间上去.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">target</span>
<span class="n">normalized_range</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">normalized_range</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">clf</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># =&gt; 0.9718804920913884
</span></code></pre></div></div><p>就是这样的简单, 基本上默认的参数可以保证很高的精度, 达到了0.97以上.再尝试用我们自己的函数:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initial_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">weights</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">initial_weights</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">iters</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">accuracy</span><span class="p">(</span><span class="n">classify</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">)),</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># =&gt; 0.9736379613356766
</span></code></pre></div></div><p>在使用很大的学习速率还有足够的迭代次数之后, 也能达到较高的精度. (当然这里还没有用到交叉验证, 用了之后我们的模型应该过学习的很厉害. 这一点这篇文章里暂时不考虑.)</p><p>上述代码的原版在这里<a href="https://github.com/simcookies/algorithm_implement/blob/master/ml/Logistic_Regression_scratch.ipynb" target="_blank" rel="noopener noreferrer">Github</a>, 以上就是对逻辑回归算法的一的简单总结.</p></div><hr><div class="post-nav"><a href="/2021/03/13/summary-of-machine-learning-algorithms-decision-tree-c4.5" class="left link">&lt;&lt; Previous </a><a href="/2021/07/17/summary-of-machine-learning-algorithms-linear-model-1" class="right link">Next &gt;&gt;</a></div><hr><div id="disqus_thread"></div><script>/**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
  */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function() {  // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');

  s.src = '//simcookiesgithubio.disqus.com/embed.js';

  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="noopener noreferrer" target="_blank">comments powered by Disqus.</a></noscript></article><div class="mark-link"><a id="bookmark"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-book fa-stack-1x fa-inverse"></i></span></a></div><div id="toc-area" class="sidebar" style="display:none"><div class="col toc-area">  <i class="fa fa-bookmark" aria-hidden="true"></i>  Table of Contents<ul class="section-nav"><li class="toc-entry toc-h1"><a href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92">逻辑回归</a><ul><li class="toc-entry toc-h2"><a href="#logit-%E5%87%BD%E6%95%B0%E5%92%8C-sigmoid-%E5%87%BD%E6%95%B0">Logit 函数和 Sigmoid 函数</a></li><li class="toc-entry toc-h2"><a href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0">逻辑回归的算法描述</a></li></ul></li><li class="toc-entry toc-h1"><a href="#%E5%9F%BA%E4%BA%8E-python-scratch-%E7%9A%84%E5%AE%9E%E7%8E%B0">基于 Python Scratch 的实现</a></li><li class="toc-entry toc-h1"><a href="#%E5%9F%BA%E4%BA%8E-scikit-learn-%E7%9A%84%E5%AE%9E%E7%8E%B0">基于 Scikit-Learn 的实现</a></li></ul></div></div></div></div><footer class="site-footer">© 2016~2021 by Simcookies. All rights reserved.<div class="quick-link" id="quick-link"><a id="page-top"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-arrow-up fa-stack-1x fa-inverse"></i> </span></a><a href="/"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-home fa-stack-1x fa-inverse"></i></span></a></div></footer></div><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-85209586-1', 'auto');
  ga('send', 'pageview');</script></body></html>