<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>机器学习算法系列 - 决策树C4.5算法</title><meta name="description" content="上一篇文章机器学习算法系列 - 决策树ID3算法, 对决策树中非常典型的 ID3 算法做了深入介绍. 这篇文章本来会大概总结一下 C4.5 算法. 但是由于 C4.5 算法与 ID3 算法高度的一致, 所以这里只谈及一些前者相对于后者的改进."><link rel="shortcut icon" type="image/x-icon" href="/public/favicon.ico"><link rel="stylesheet" href="/public/css/main.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.5.0/css/font-awesome.css" integrity="sha512-EaaldggZt4DPKMYBa143vxXQqLq5LE29DG/0OoVenoyxDrAScYrcYcHIuxYO9YNTIQMgD8c8gIUU8FQw7WpXSQ==" crossorigin="anonymous"><link rel="canonical" href="https://simcookies.github.io/2021/03/13/summary-of-machine-learning-algorithms-decision-tree-c4.5"><link rel="alternate" type="application/rss+xml" title="Simcookies" href="https://simcookies.github.io/feed.xml"><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" integrity="sha512-uto9mlQzrs59VwILcLiRYeLKPPbS/bT71da/OEBYEwcdNUk8jYIy+D176RYoop1Da+f9mvkYrmj5MCLZWEtQuA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.7.12/simple-jekyll-search.min.js" integrity="sha512-APy7Ff/y4pdIHleqiTMcRZ8Wu6tjfqhJpgAcaCvTuFQPj/G+/A5u0uZi/DkfkuLQ6FeFmDJgh/zl0y3If/VUyA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.2/mermaid.min.js" integrity="sha512-x8NWYlEsC86ngfO24tbxW6pMuyn9gYnwEW0FcSobohDc3iLCXhmRkqYXgTfE7Jwy2YCTnHRfyum8LUIiyvmgWQ==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha512-Fv0JVzxvrWFLhiUP8wZYR6VwBZQt0XqB9MhHZE+12lObqIDzg3LOJcCCiRy6g4MCX/MlG+R9IRCWZAHjo9iBgw==" crossorigin="anonymous"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      showProcessingMessages: false,
      messageStyle: "none",
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
      }
    });</script><script src="/public/script/main.js"></script></head><body><header class="site-header"><div class="wrapper"><a class="site-title" href="/">Simcookies</a> <span class="menu-icon" id="menu-icon"><span class="fa-stack fa-lg"><i class="fa fa-square-o fa-stack-2x"></i> <i class="fa fa-bars fa-stack-1x"></i></span></span><nav class="site-nav"><div class="trigger"><a class="page-link" href="/about/">About</a> <a class="page-link" href="/archive/">Archive</a> <a class="page-link" href="/wiki/">Wiki</a> <a class="page-link" href="/feed.xml">Feeds</a> <a class="page-link" href="https://github.com/simcookies/gitio" target="_blank" rel="noopener noreferrer"><i class="fa fa-github-alt fa-lg"></i> Repositories</a></div></nav></div></header><div class="scroll-content"><div class="page-content"><div class="wrapper"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习算法系列 - 决策树C4.5算法</h1><p class="post-meta"><time datetime="2021-03-13T12:30:31+09:00" itemprop="datePublished">Mar 13, 2021</time>  <i class="fa fa-archive" aria-hidden="true"></i>  <span><a class="category-div" href="/category/machine-learning">machine learning</a></span>  <i class="fa fa-tag"></i>  <a class="tag-div" href="/tag/algorithm">algorithm</a>  <a class="tag-div" href="/tag/formula">formula</a> </p></header><div class="post-content" itemprop="articleBody"><p>上一篇文章<a href="/2020/11/12/summary-of-machine-learning-algorithms-decision-tree-id3">机器学习算法系列 - 决策树ID3算法</a>, 对决策树中非常典型的 ID3 算法做了深入介绍. 这篇文章本来会大概总结一下 C4.5 算法. 但是由于 C4.5 算法与 ID3 算法高度的一致, 所以这里只谈及一些前者相对于后者的改进.</p><h1 id="提出问题">提出问题</h1><p>ID3 算法的核心在于找到信息增益最大的特征, 在上一篇文章的例子当中, 就能感觉到其非常直观和易于理解. 但是在一些特殊的情况下, 单是绝对的信息增益是不能够找到真正最有用的特征的.</p><p>假设训练数据集中有一列 ID, 每条记录都有一个独立的 ID. 那么这个 ID 列的条件熵会变成 0 (因为在 ID 确定的条件下, 出现的类别数只有一个). 使用这个值为 0 的条件熵得到的信息增益就会非常大 (等于原来的信息熵). 从现实意义上看, 这也是符合的. 因为一条记录如果确定了它的 ID, 那么他的类别一定也是能被确定的. 原因就是 ID 这个信息里面包含了太多的信息量了. 但是显然不能使用 ID 作为一个节点的特征. 实际上, 对于某些特征来说, 其每个属性所包含的每个分类类别很多的时候, 都会得到很大信息增益. 这个时候只通过信息增益是不能选出有效的特征的.</p><p>概括地说, 就是一个特征其特征熵很大的时候信息增益就很大, 反之就很小.</p><h1 id="解决方法">解决方法</h1><p>为了排除特征本身信息熵地干扰, C4.5 算法选择了信息增益率 (Gain Ratio) 作为选择分支的准则。对于特征熵很大的特征，除上一个相应的较大的数值，反之特征熵小的特征除一个较小的数值，使得特征的信息熵的影响变小。这里 C4.5 算法中所除以的数值使用了属性熵。</p>\[H_A(D) = -\sum\limits_{i=1}^{n}\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}\notag\]<p>当数据集 $D$ 中 $A$ 属性里有很多不同的取值时，上述的属性熵就会变得很大。这样由信息增益除以该值所得到的增益率就能够正真反映出属性 $A$ 作为一个特征，区分样本的能力。<strong>最终就避免了 ID3 趋向与选择取值较多的属性作为节点的问题</strong>。</p><h2 id="描述">描述</h2><ol><li>从根节点开始, 对节点计算所有可能的特征的信息增益;</li><li>选择<strong>信息增益比</strong>最大的特征作为节点的特征, 构建子节点;</li><li>对子节点递归调用上述方法, 构建决策树;</li><li>直到所有特征的信息增益均很小 (可用事前决定的阈值进行判断) 或者没有特征可以选择</li></ol><h2 id="步骤">步骤</h2><p>输入: 训练数据集 $D$, 特征集 $A$, 阈值 $\epsilon$<br>输出: 决策树 $T$<br>步骤:</p><ol><li>若 $D$ 中所有实例输入同一类 $C_k$, 则 $T$ 为单节点树, 并将类 $C_k$ 作为该节点的类标记, 返回 $T$; (只有一个类的情况)</li><li>若 $A=\varnothing$ , 则 $T$ 为单节点树, 并将 $D$ 实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$; (没有特征量的情况)</li><li>否则, 计算 $A$ 中各个特征对于 $D$ 的<strong>信息增益比</strong>, 选择信息增益比最大的特征 $A_g$;</li><li>如果 $A_g$ 的信息增益比小于阈值 $\epsilon$, 则置 $T$ 为单节点树, 并将 $D$ 中实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$;</li><li>否则, 对 $A_g$ 的每一个可能值 $a_i$, 依 $A_g = a_i$ 将 $D$ 分割为若干个非空子集 $D_i$, 将 $D_i$ 中实例数最大的类作为标记, 构建子节点, 由节点及其子节点构成树 $T$, 返回 $T$;</li><li>对于第 $i$ 个子节点, 以 $D_i$ 为训练集, 以 $A-{A_g}$ 为特征集, 递归调用步1 ~ 步5, 得到子树 $T_i$, 返回 $T_i$.</li></ol><p>可以看出 C4.5 算法和 ID3 算法除了特征选择的参照物不同之外，其他基本上是一致的。</p><h1 id="基于-python-scratch-的实现">基于 Python Scratch 的实现</h1><p>对于同一个问题<a href="/2020/11/12/summary-of-machine-learning-algorithms-decision-tree-id3#%E9%97%AE%E9%A2%98%E6%8F%90%E5%87%BA">预测能否贷款</a>，和前文的ID3一样，这里也给出 Python Scratch 的实现。代码上C4.5算法和ID3算法也是一致的，只是增加了计算属性熵的部分，并且原来取得最大信息增益的函数需要改为计算最大信息增益率的函数。</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_best_gain_ratio</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
    <span class="n">h_d</span> <span class="o">=</span> <span class="n">calc_entropy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">gr_max</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">feature_gr_max</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
        <span class="n">x_feature</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">ps</span> <span class="o">=</span> <span class="n">calc_probs</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)</span>
        <span class="n">Hs</span> <span class="o">=</span> <span class="p">[</span><span class="n">calc_entropy</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">x_feature</span> <span class="o">==</span> <span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)]</span>
        <span class="n">h_da</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">Hs</span><span class="p">))</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">h_d</span> <span class="o">-</span> <span class="n">h_da</span>
        <span class="c1"># 计算属性熵
</span>        <span class="n">h_ad</span> <span class="o">=</span> <span class="n">calc_entropy</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)</span>
        <span class="n">gr</span> <span class="o">=</span> <span class="n">g</span> <span class="o">/</span> <span class="n">h_ad</span>
        <span class="k">if</span> <span class="n">gr</span> <span class="o">&gt;</span> <span class="n">gr_max</span><span class="p">:</span>
            <span class="n">gr_max</span> <span class="o">=</span> <span class="n">gr</span>
            <span class="n">feature_gr_max</span> <span class="o">=</span> <span class="n">i</span>
    <span class="k">return</span> <span class="n">gr_max</span><span class="p">,</span> <span class="n">feature_gr_max</span>
</code></pre></div></div><p>最终版的代码在这里 <a href="https://github.com/simcookies/algorithm_implement/blob/master/ml/Decision_Tree_C4.5_scratch.ipynb" target="_blank" rel="noopener noreferrer">Github</a>。scikit-learn 的实现和之前的一篇完全一样，这里不再赘述。另外根据 scikit-learn 的官方文档，决策树中采用的并不是 ID3/C4.5 算法，而是 CART。（这就很尴尬😅了，不过知道算法的实现终归是好事。）</p><blockquote><p>scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.</p></blockquote><hr><p>以上就是关于 C4.5 算法的说明。之后算法的发明者 Ross Quinlan 将算法更新到了 C5.0（有专利保护），使得计算更快，结果更精确，不过这就是另外的话了。</p></div><hr><div class="post-nav"><a href="/2020/11/12/summary-of-machine-learning-algorithms-decision-tree-id3" class="left link">&lt;&lt; Previous </a><a href="/2021/05/09/summary-of-machine-learning-algorithms-logistic-regression" class="right link">Next &gt;&gt;</a></div><hr><div id="disqus_thread"></div><script>/**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
  */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function() {  // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');

  s.src = '//simcookiesgithubio.disqus.com/embed.js';

  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="noopener noreferrer" target="_blank">comments powered by Disqus.</a></noscript></article></div></div><footer class="site-footer">© 2016~2021 by Simcookies. All rights reserved.<div class="quick-link" id="quick-link"><a id="page-top"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-arrow-up fa-stack-1x fa-inverse"></i> </span></a><a href="/"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-home fa-stack-1x fa-inverse"></i></span></a></div></footer></div><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-85209586-1', 'auto');
  ga('send', 'pageview');</script></body></html>