<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>机器学习算法系列 - 决策树ID3算法</title><meta name="description" content="在上一篇文章&lt;机器学习算法系列 - 决策树(序)&gt;中介绍了三个决策学习的算法,  ID3, C4.5, CART. 这一篇文章对 ID3 作深一点的介绍, 并且基于 Scratch Python 以及 Scikit-learn 包进行实现."><link rel="shortcut icon" type="image/x-icon" href="/public/favicon.ico"><link rel="stylesheet" href="/public/css/main.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.5.0/css/font-awesome.css" integrity="sha512-EaaldggZt4DPKMYBa143vxXQqLq5LE29DG/0OoVenoyxDrAScYrcYcHIuxYO9YNTIQMgD8c8gIUU8FQw7WpXSQ==" crossorigin="anonymous"><link rel="canonical" href="https://simcookies.github.io/2020/11/12/summary-of-machine-learning-algorithms-decision-tree-id3"><link rel="alternate" type="application/rss+xml" title="Simcookies" href="https://simcookies.github.io/feed.xml"><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" integrity="sha512-uto9mlQzrs59VwILcLiRYeLKPPbS/bT71da/OEBYEwcdNUk8jYIy+D176RYoop1Da+f9mvkYrmj5MCLZWEtQuA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.7.12/simple-jekyll-search.min.js" integrity="sha512-APy7Ff/y4pdIHleqiTMcRZ8Wu6tjfqhJpgAcaCvTuFQPj/G+/A5u0uZi/DkfkuLQ6FeFmDJgh/zl0y3If/VUyA==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.2/mermaid.min.js" integrity="sha512-x8NWYlEsC86ngfO24tbxW6pMuyn9gYnwEW0FcSobohDc3iLCXhmRkqYXgTfE7Jwy2YCTnHRfyum8LUIiyvmgWQ==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha512-Fv0JVzxvrWFLhiUP8wZYR6VwBZQt0XqB9MhHZE+12lObqIDzg3LOJcCCiRy6g4MCX/MlG+R9IRCWZAHjo9iBgw==" crossorigin="anonymous"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      showProcessingMessages: false,
      messageStyle: "none",
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
      }
    });</script><script src="/public/script/main.js"></script></head><body><header class="site-header"><div class="wrapper"><a class="site-title" href="/">Simcookies</a> <span class="menu-icon" id="menu-icon"><span class="fa-stack fa-lg"><i class="fa fa-square-o fa-stack-2x"></i> <i class="fa fa-bars fa-stack-1x"></i></span></span><nav class="site-nav"><div class="trigger"><a class="page-link" href="/about/">About</a> <a class="page-link" href="/archive/">Archive</a> <a class="page-link" href="/wiki/">Wiki</a> <a class="page-link" href="/feed.xml">Feeds</a> <a class="page-link" href="https://github.com/simcookies/gitio" target="_blank" rel="noopener noreferrer"><i class="fa fa-github-alt fa-lg"></i> Repositories</a></div></nav></div></header><div class="scroll-content"><div class="page-content"><div class="wrapper"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习算法系列 - 决策树ID3算法</h1><p class="post-meta"><time datetime="2020-11-12T22:02:31+09:00" itemprop="datePublished">Nov 12, 2020</time>  <i class="fa fa-archive" aria-hidden="true"></i>  <span><a class="category-div" href="/category/machine-learning">machine learning</a></span>  <i class="fa fa-tag"></i>  <a class="tag-div" href="/tag/algorithm">algorithm</a>  <a class="tag-div" href="/tag/formula">formula</a> </p></header><div class="post-content" itemprop="articleBody"><p>在上一篇文章&lt;<a href="/2020/11/02/summary-of-machine-learning-algorithms-decision-tree">机器学习算法系列 - 决策树(序)</a>&gt;中介绍了三个决策学习的算法, ID3, C4.5, CART. 这一篇文章对 ID3 作深一点的介绍, 并且基于 Scratch Python 以及 Scikit-learn 包进行实现.</p><h1 id="id3-的再介绍">ID3 的再介绍</h1><p>ID3 全称是 Iterative Dichotomiser 3 (迭代二叉树 3 代), 是由 <a href="https://en.wikipedia.org/wiki/Ross_Quinlan" target="_blank" rel="noopener noreferrer">Ross Quinlan</a> 发明的用于决策树的算法. 该算法基于信息熵和信息增益的概念, 能够生成用于分类的决策树. 其算法的语言描述为:</p><h2 id="描述">描述</h2><ol><li>从根节点开始, 对节点计算所有可能的特征的信息增益;</li><li>选择信息增益最大的特征作为节点的特征, 构建子节点;</li><li>对子节点递归调用上述方法, 构建决策树;</li><li>直到所有特征的信息增益均很小 (可用事前决定的阈值进行判断) 或者没有特征可以选择</li></ol><h2 id="步骤">步骤</h2><p>输入: 训练数据集 $D$, 特征集 $A$, 阈值 $\epsilon$<br>输出: 决策树 $T$<br>步骤:</p><ol><li>若 $D$ 中所有实例输入同一类 $C_k$, 则 $T$ 为单节点树, 并将类 $C_k$ 作为该节点的类标记, 返回 $T$; (只有一个类的情况)</li><li>若 $A=\varnothing$ , 则 $T$ 为单节点树, 并将 $D$ 实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$; (没有特征量的情况)</li><li>否则, 计算 $A$ 中各个特征对于 $D$ 的信息增益, 选择信息增益最大的特征 $A_g$;</li><li>如果 $A_g$ 的信息增益小于阈值 $\epsilon$, 则置 $T$ 为单节点树, 并将 $D$ 中实例数最大的类 $C_k$ 作为该节点的类标记, 返回 $T$;</li><li>否则, 对 $A_g$ 的每一个可能值 $a_i$, 依 $A_g = a_i$ 将 $D$ 分割为若干个非空子集 $D_i$, 将 $D_i$ 中实例数最大的类作为标记, 构建子节点, 由节点及其子节点构成树 $T$, 返回 $T$;</li><li>对于第 $i$ 个子节点, 以 $D_i$ 为训练集, 以 $A-{A_g}$ 为特征集, 递归调用步1 ~ 步5, 得到子树 $T_i$, 返回 $T_i$.</li></ol><h1 id="问题提出">问题提出</h1><p>通过一个人的年龄, 工作状况, 是否有房子以及信贷状况等信息预测他能否申请贷款. 数据如下:</p><table><thead><tr><th>ID</th><th>年龄</th><th>有工作</th><th>有自己的房子</th><th>信贷状况</th><th>类别</th></tr></thead><tbody><tr><td>1</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>2</td><td>青年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>3</td><td>青年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>4</td><td>青年</td><td>是</td><td>是</td><td>一般</td><td>是</td></tr><tr><td>5</td><td>青年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>6</td><td>中年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr><tr><td>7</td><td>中年</td><td>否</td><td>否</td><td>好</td><td>否</td></tr><tr><td>8</td><td>中年</td><td>是</td><td>是</td><td>好</td><td>是</td></tr><tr><td>9</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>10</td><td>中年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>11</td><td>老年</td><td>否</td><td>是</td><td>非常好</td><td>是</td></tr><tr><td>12</td><td>老年</td><td>否</td><td>是</td><td>好</td><td>是</td></tr><tr><td>13</td><td>老年</td><td>是</td><td>否</td><td>好</td><td>是</td></tr><tr><td>14</td><td>老年</td><td>是</td><td>否</td><td>非常好</td><td>是</td></tr><tr><td>15</td><td>老年</td><td>否</td><td>否</td><td>一般</td><td>否</td></tr></tbody></table><p>使用 ID3 算法生成可用于分类的决策树.</p><h1 id="基于-python-scratch-的实现">基于 Python Scratch 的实现</h1><p>一共有 4 种特征量 <code class="language-plaintext highlighter-rouge">年龄</code>, <code class="language-plaintext highlighter-rouge">工作状况</code>, <code class="language-plaintext highlighter-rouge">是否有房子</code>, <code class="language-plaintext highlighter-rouge">信贷状况</code>.</p><blockquote><p>其实不能算得上真正的 Scratch, 毕竟用了 Numpy <img class="emoji" title=":sweat_smile:" alt=":sweat_smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png" height="20" width="20"></p></blockquote><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">feature_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">"年龄"</span><span class="p">,</span> <span class="s">"是否有工作"</span><span class="p">,</span> <span class="s">"是否有自己的房子"</span><span class="p">,</span> <span class="s">"信贷状况"</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="s">"青年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"一般"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"青年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"好"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"青年"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"好"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"青年"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"一般"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"青年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"一般"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"中年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"一般"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"中年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"好"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"中年"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"好"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"中年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"非常好"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"中年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"非常好"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"老年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"非常好"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"老年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"好"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"老年"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"好"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"老年"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"非常好"</span><span class="p">,</span> <span class="s">"是"</span><span class="p">],</span>
        <span class="p">[</span><span class="s">"老年"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">,</span> <span class="s">"一般"</span><span class="p">,</span> <span class="s">"否"</span><span class="p">],</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span> <span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div><p>有两个函数需要经常用到, 可以事先定义好.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 计算 data 数据中, 各个类别出现的概率
</span><span class="k">def</span> <span class="nf">calc_probs</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">data</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">probs</span>

<span class="c1"># 计算 data 的信息熵
</span><span class="k">def</span> <span class="nf">calc_entropy</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">_ps</span> <span class="o">=</span> <span class="n">calc_probs</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">entropy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="o">-</span><span class="n">pi</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span> <span class="k">for</span> <span class="n">pi</span> <span class="ow">in</span> <span class="n">_ps</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">entropy</span>
</code></pre></div></div><p>直接对 <code class="language-plaintext highlighter-rouge">y_train</code> 计算熵 $H(D)$:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">H_D</span> <span class="o">=</span> <span class="n">calc_entropy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="c1"># =&gt; 0.9709505944546686
</span></code></pre></div></div><p>计算并且选择出拥有最大信息增益的特征:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">g_max</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">feature_g_max</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">x_feature</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">calc_probs</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)</span>
    <span class="n">Hs</span> <span class="o">=</span> <span class="p">[</span><span class="n">calc_entropy</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">x_feature</span> <span class="o">==</span> <span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)]</span>
    <span class="n">H_DA</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">Hs</span><span class="p">))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">H_D</span> <span class="o">-</span> <span class="n">H_DA</span>
    <span class="k">if</span> <span class="n">g</span> <span class="o">&gt;</span> <span class="n">g_max</span><span class="p">:</span>
        <span class="n">g_max</span> <span class="o">=</span> <span class="n">g</span>
        <span class="n">feature_g_max</span> <span class="o">=</span> <span class="n">i</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"最大信息增益: </span><span class="si">{</span><span class="n">g_max</span><span class="si">}</span><span class="s">, 对应特征: </span><span class="si">{</span><span class="n">feature_g_max</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="c1"># =&gt; 最大信息增益: 0.4199730940219749, 对应特征: 2
</span></code></pre></div></div><p>这样第一个循环下来, 就得到了根节点的特征为 2, 也就是 <code class="language-plaintext highlighter-rouge">是否有自己的房子</code> 这个特征. 这样决策树就变成了这样:</p><div class="mermaid">graph TD
root[是否有自己的房子?]-- 没有 --&gt;N1[ID: 1,2,3,5,6,7,13,14,15]
root-- 有 --&gt;N2[ID: 4,8,9,10,11,12]
</div><p>确定了根节点后, 就得到了两个子节点. 就可以再次进入循环确定节点的特征. 比如对于上图中左边的节点, 需要考察的特征为 $A-{A_g}$ 也就是除了房子状况之外的3个特征.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 更新数据
</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[:,</span> <span class="n">feature_g_max</span><span class="p">]</span> <span class="o">==</span> <span class="s">"否"</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span> <span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">H_D</span> <span class="o">=</span> <span class="n">calc_entropy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 去掉已经用掉的特征
</span><span class="n">features</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">feature_g_max</span><span class="p">)</span>
<span class="n">g_max</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">feature_g_max</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
    <span class="n">x_feature</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">calc_probs</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)</span>
    <span class="n">Hs</span> <span class="o">=</span> <span class="p">[</span><span class="n">calc_entropy</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">x_feature</span> <span class="o">==</span> <span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)]</span>
    <span class="n">H_DA</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">Hs</span><span class="p">))</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">H_D</span> <span class="o">-</span> <span class="n">H_DA</span>
    <span class="k">if</span> <span class="n">g</span> <span class="o">&gt;</span> <span class="n">g_max</span><span class="p">:</span>
        <span class="n">g_max</span> <span class="o">=</span> <span class="n">g</span>
        <span class="n">feature_g_max</span> <span class="o">=</span> <span class="n">i</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"最大信息增益: </span><span class="si">{</span><span class="n">g_max</span><span class="si">}</span><span class="s">, 对应特征: </span><span class="si">{</span><span class="n">feature_g_max</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="c1"># =&gt; 最大信息增益: 0.9182958340544896, 对应特征: 1
</span></code></pre></div></div><p>这样左边的节点就选择了特征 1 也就是 <code class="language-plaintext highlighter-rouge">是否有工作</code> 作为其特征. 决策树就更新为:</p><div class="mermaid">graph TD
root[是否有自己的房子?]-- 没有 --&gt;N1[是否有工作?]
root-- 有 --&gt;N2[ID: 4,8,9,10,11,12]
N1 -- 没有 --&gt; N3[ID: 1,2,5,6,7,15]
N1 -- 有 --&gt; N4[ID: 3,13,14]
</div><p>左边的节点可以继续迭代往下生长. 再来看右边的节点.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 更新数据
</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[:,</span> <span class="n">feature_g_max</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span> <span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">)))</span>
<span class="c1"># =&gt; 1
</span></code></pre></div></div><p>这些 ID 对应的人只有一个类别, 即都是可以申请贷款的, 因此右节点确定为类标记 “1” 之后就结束了. 之后的决策树更新为:</p><div class="mermaid">graph TD
root[是否有自己的房子?]-- 没有 --&gt;N1[是否有工作?]
root-- 有 --&gt;N2(可以申请贷款)
style N2 fill:#ff0
N1 -- 没有 --&gt; N3[ID: 1,2,5,6,7,15]
N1 -- 有 --&gt; N4[ID: 3,13,14]
</div><p>上述的操作按照算法描述可以写在一个迭代函数中 <code class="language-plaintext highlighter-rouge">ID3_tree_generate(data, features, e=0.01)</code>.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 计算并返回最大信息增益和特征
</span><span class="k">def</span> <span class="nf">find_best_gain</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
    <span class="n">H_D</span> <span class="o">=</span> <span class="n">calc_entropy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">g_max</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">feature_g_max</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
        <span class="n">x_feature</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">ps</span> <span class="o">=</span> <span class="n">calc_probs</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)</span>
        <span class="n">Hs</span> <span class="o">=</span> <span class="p">[</span><span class="n">calc_entropy</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">x_feature</span> <span class="o">==</span> <span class="n">j</span><span class="p">])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x_feature</span><span class="p">)]</span>
        <span class="n">H_DA</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">ps</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">Hs</span><span class="p">))</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">H_D</span> <span class="o">-</span> <span class="n">H_DA</span>
        <span class="k">if</span> <span class="n">g</span> <span class="o">&gt;</span> <span class="n">g_max</span><span class="p">:</span>
            <span class="n">g_max</span> <span class="o">=</span> <span class="n">g</span>
            <span class="n">feature_g_max</span> <span class="o">=</span> <span class="n">i</span>
    <span class="k">return</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">feature_g_max</span>


<span class="c1"># ID3 迭代算法
</span><span class="k">def</span> <span class="nf">ID3_tree_generate</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">e</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Step 1
</span>    <span class="c1"># 数据都是一个类别时
</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Step 2
</span>    <span class="c1"># 特征的数量为0时
</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_train</span><span class="p">).</span><span class="n">argmax</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Step 3
</span>    <span class="c1"># 计算最大信息增益和特征
</span>    <span class="n">g_max</span><span class="p">,</span> <span class="n">feature_g_max</span> <span class="o">=</span> <span class="n">find_best_gain</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>

    <span class="c1"># Step 4
</span>    <span class="c1"># 最大信息增益比阈值小时
</span>    <span class="k">if</span> <span class="n">g_max</span> <span class="o">&lt;</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">tree</span> <span class="o">=</span> <span class="p">{</span><span class="n">feature_columns</span><span class="p">[</span><span class="n">feature_g_max</span><span class="p">]:</span> <span class="n">np</span><span class="p">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_train</span><span class="p">).</span><span class="n">argmax</span><span class="p">()}</span>
        <span class="k">return</span> <span class="n">tree</span>

    <span class="c1"># Step 5, 6
</span>    <span class="c1"># 分类迭代生成子树
</span>    <span class="n">splited_datas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="n">feature_g_max</span><span class="p">])</span>
    <span class="n">tree</span> <span class="o">=</span> <span class="p">{</span><span class="n">feature_columns</span><span class="p">[</span><span class="n">feature_g_max</span><span class="p">]:</span> <span class="p">{}}</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">splited_datas</span><span class="p">:</span>
        <span class="n">sub_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[:,</span> <span class="n">feature_g_max</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">sub_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">sub_features</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">feature_g_max</span><span class="p">)</span>
        <span class="n">tree</span><span class="p">[</span><span class="n">feature_columns</span><span class="p">[</span><span class="n">feature_g_max</span><span class="p">]][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ID3_tree_generate</span><span class="p">(</span>
            <span class="n">sub_data</span><span class="p">,</span> <span class="n">sub_features</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">tree</span>
</code></pre></div></div><p>最后, 直接调用 <code class="language-plaintext highlighter-rouge">ID3_tree_generate</code> 生成决策树.</p><blockquote><p>本例使用了 dict 存储树</p></blockquote><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">ID3_tree_generate</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
<span class="c1"># =&gt; {'是否有自己的房子': {'否': {'是否有工作': {'否': '否', '是': '是'}}, '是': '是'}}
</span></code></pre></div></div><p>最终的树就成了:</p><div class="mermaid">graph TD
root[是否有自己的房子?]-- 没有 --&gt;N1[是否有工作?]
root-- 有 --&gt;N2(可以申请贷款)
style N2 fill:#ff0
N1 -- 没有 --&gt; N3(不可以申请贷款)
style N3 fill:#ff0
N1 -- 有 --&gt; N4(可以申请贷款)
style N4 fill:#ff0
</div><p>这样就完成了一个可用于预测的决策树模型了. 单从这个例子的数据看, 可以发现房子和工作才是能否贷款的最主要原因, 和年龄状况/信贷状况没有太大关系. 另外需要注意的是, 因为这里模型非常的简单, 所以没有用到树的修剪, 阈值的设置也没有起到什么作用.</p><h1 id="基于-scikit-learn-的实现">基于 Scikit-Learn 的实现</h1><p>scikit-learn 的 Tree 提供了 <code class="language-plaintext highlighter-rouge">DecisionTreeClassifier</code> 用于决策学习. 使用同样的数据来实现.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># scikit-learn 需要数字进行处理, 当然也可以使用 OneHotEncoder
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s">"entropy"</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div><p>这里特别指定了 <code class="language-plaintext highlighter-rouge">criterion</code> 为 <code class="language-plaintext highlighter-rouge">entropy</code> , 是因为 scikit-learn 默认采用了基尼指数作为指标. 可以使用 <code class="language-plaintext highlighter-rouge">pydotplus</code> 将训练好的树进行可视化.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">pydotplus</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span>

<span class="n">dot_data</span> <span class="o">=</span> <span class="n">export_graphviz</span><span class="p">(</span>
    <span class="n">clf</span><span class="p">,</span>
    <span class="n">out_file</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">rounded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_columns</span><span class="p">,</span>  
    <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s">"能贷款"</span><span class="p">,</span> <span class="s">"不能贷款"</span><span class="p">],</span>
    <span class="n">special_characters</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">pydotplus</span><span class="p">.</span><span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span>
<span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="p">.</span><span class="n">create_png</span><span class="p">())</span>
</code></pre></div></div><p><img src="https://raw.githubusercontent.com/simcookies/image-host/master/imgs/20201113191853.png" alt="决策树"></p><p>可以看出 scikit-learn 得到的树和上面 scratch 得到树是一样的. 最终版的代码在这里 <a href="https://github.com/simcookies/algorithm_implement/blob/master/ml/Decision_Tree_ID3_scratch.ipynb" target="_blank" rel="noopener noreferrer">Github</a>。到这里, 关于 ID3 简单的实现就结束了.</p></div><hr><div class="post-nav"><a href="/2020/11/02/summary-of-machine-learning-algorithms-decision-tree" class="left link">&lt;&lt; Previous </a><a href="/2021/03/13/summary-of-machine-learning-algorithms-decision-tree-c4.5" class="right link">Next &gt;&gt;</a></div><hr><div id="disqus_thread"></div><script>/**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
  */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function() {  // DON'T EDIT BELOW THIS LINE
  var d = document, s = d.createElement('script');

  s.src = '//simcookiesgithubio.disqus.com/embed.js';

  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="noopener noreferrer" target="_blank">comments powered by Disqus.</a></noscript></article><div class="mark-link"><a id="bookmark"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-book fa-stack-1x fa-inverse"></i></span></a></div><div id="toc-area" class="sidebar" style="display:none"><div class="col toc-area">  <i class="fa fa-bookmark" aria-hidden="true"></i>  Table of Contents<ul class="section-nav"><li class="toc-entry toc-h1"><a href="#id3-%E7%9A%84%E5%86%8D%E4%BB%8B%E7%BB%8D">ID3 的再介绍</a><ul><li class="toc-entry toc-h2"><a href="#%E6%8F%8F%E8%BF%B0">描述</a></li><li class="toc-entry toc-h2"><a href="#%E6%AD%A5%E9%AA%A4">步骤</a></li></ul></li><li class="toc-entry toc-h1"><a href="#%E9%97%AE%E9%A2%98%E6%8F%90%E5%87%BA">问题提出</a></li><li class="toc-entry toc-h1"><a href="#%E5%9F%BA%E4%BA%8E-python-scratch-%E7%9A%84%E5%AE%9E%E7%8E%B0">基于 Python Scratch 的实现</a></li><li class="toc-entry toc-h1"><a href="#%E5%9F%BA%E4%BA%8E-scikit-learn-%E7%9A%84%E5%AE%9E%E7%8E%B0">基于 Scikit-Learn 的实现</a></li></ul></div></div></div></div><footer class="site-footer">© 2016~2021 by Simcookies. All rights reserved.<div class="quick-link" id="quick-link"><a id="page-top"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-arrow-up fa-stack-1x fa-inverse"></i> </span></a><a href="/"><span class="fa-stack fa-lg"><i class="fa fa-square fa-stack-2x"></i> <i class="fa fa-home fa-stack-1x fa-inverse"></i></span></a></div></footer></div><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                           m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-85209586-1', 'auto');
  ga('send', 'pageview');</script></body></html>